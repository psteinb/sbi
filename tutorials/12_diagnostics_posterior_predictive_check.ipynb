{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3915927-a52e-4918-a8f9-f2cd82331e13",
   "metadata": {},
   "source": [
    "# Posterior Predictive Checks (PPC) in SBI\n",
    "\n",
    "A common **safety check** performed as part of inference are [Posterior Predictive Checks (PPC)](https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssa.12378). A PPC compares data $x_{\\text{pp}}$ generated using the parameters $\\theta_{\\text{posterior}}$ sampled from the posterior with the observed data $x_o$. The general concept is that -if the inference is correct- **the generated data $x_{\\text{pp}}$ should \"look similar\" to the oberved data $x_0$**. Said differently, $x_o$ should be within the support of $x_{\\text{pp}}$.\n",
    "\n",
    "A PPC usually should not be used as a [validation metric](http://proceedings.mlr.press/v130/lueckmann21a.html). Nonetheless a PPC is a good start for an inference diagnosis and can provide an intuition about any bias introduced in inference: does $x_{\\text{pp}}$ systematically differ from $x_o$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ec374e-949a-47bb-951d-5f6c637bb283",
   "metadata": {},
   "source": [
    "## Conceptual Code for PPC\n",
    "\n",
    "The following illustrates the main approach of PPCs. We have a trained neural posterior and want to check the correlation between the observation(s) $x_o$ and the posterior sample(s) $x_{\\text{pp}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fdd318-0b93-41f7-a886-b39f2b374f3a",
   "metadata": {},
   "source": [
    "```python\n",
    "from sbi.analysis import pairplot\n",
    "\n",
    "# A PPC is performed after we trained a neural posterior `posterior`\n",
    "posterior.set_default_x(x_o) # x_o loaded from disk for example\n",
    "\n",
    "# We draw theta samples from the posterior. This part is not in the scope of SBI\n",
    "posterior_samples = posterior.sample((5_000,))\n",
    "\n",
    "# We use posterior theta samples to generate x data\n",
    "x_pp = simulator(posterior_samples)\n",
    "\n",
    "# We verify if the observed data falls within the support of the generated data\n",
    "_ = pairplot(\n",
    "    samples=x_pp,\n",
    "    points=x_o\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7064e4-4b1d-4f44-870a-2b81e4a72452",
   "metadata": {},
   "source": [
    "## Performing a PPC of a toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36247704-135b-496e-a62b-46fbaa05678d",
   "metadata": {},
   "source": [
    "Below we provide an example Posterior Predictive Check (PPC) of some toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdca261-2ea4-438d-9fef-359d65a1c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sbi.analysis import pairplot\n",
    "\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1bd2c2-ad38-4cfb-8642-fc6c2cd5e5a2",
   "metadata": {},
   "source": [
    "We work on an inference problem over three parameters using any of the techniques implemented in `sbi`. In this tutorial, we load the dummy posterior (from a python module `toy_posterior_for_07_cc` alongside this notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd234487-7062-480e-83c1-0b8f230e95b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toy_posterior_for_07_cc import ExamplePosterior\n",
    "\n",
    "posterior = ExamplePosterior()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775324d6-b1b5-4499-8891-dbd377efc722",
   "metadata": {},
   "source": [
    "Let us say that we are observing the data point $x_o$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707857da-5a2d-4af7-996e-9a67d8cb3302",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 5  # simulator output was 5-dimensional\n",
    "x_o = torch.ones(1, D)\n",
    "posterior.set_default_x(x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb673a4a-431b-4d35-87e1-625909ea0dc4",
   "metadata": {},
   "source": [
    "The posterior can be used to draw $\\theta_{\\text{posterior}}$ samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854b258a-43d8-4834-925a-e30b36ef08ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = posterior.sample((5_000,))\n",
    "\n",
    "fig, ax = pairplot(\n",
    "    samples=posterior_samples,\n",
    "    limits=torch.tensor([[-2.5, 2.5]] * 3),\n",
    "    offdiag=[\"kde\"],\n",
    "    diag=[\"kde\"],\n",
    "    figsize=(5, 5),\n",
    "    labels=[rf\"$\\theta_{d}$\" for d in range(3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea5863-f61c-44c0-a076-1ab86d4f2008",
   "metadata": {},
   "source": [
    "Now we can use our simulator to generate some data $x_{\\text{PP}}$. We will use the poterior samples $\\theta_{\\text{posterior}}$ as input parameters. Note that the simulation part is not in the `sbi` scope, so any simulator -including a non-Python one- can be used at this stage. In our case we'll use a dummy simulator for the sake of demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27db771d-10e4-45c4-8422-6145581dd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_simulator(theta: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "    \"\"\" a function performing a simulation emulating a real simulator outside sbi\n",
    "\n",
    "    Args:\n",
    "        theta: parameters to control the simulation (in this tutorial,\n",
    "            these are the posterior_samples $\\theta_{\\text{posterior}}$ obtained\n",
    "            from the trained posterior.\n",
    "        args: parameters\n",
    "        kwargs: keyword arguments\n",
    "    \"\"\"\n",
    "\n",
    "    sample_size = theta.shape[0] # number of posterior_samples\n",
    "    scale = 1.0\n",
    "\n",
    "    shift = torch.distributions.Gumbel(loc=torch.zeros(D), scale=scale / 2).sample()\n",
    "    return torch.distributions.Gumbel(loc=x_o[0] + shift, scale=scale).sample(\n",
    "        (sample_size,)\n",
    "    )\n",
    "\n",
    "\n",
    "x_pp = dummy_simulator(posterior_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06d9afb-b3e6-4f0c-a529-fcbbd5afa0f3",
   "metadata": {},
   "source": [
    "Plotting $x_o$ against the $x_{\\text{pp}}$, we perform a PPC that represents a sanity check. In this case, the check indicates that $x_o$ falls right within the support of $x_{\\text{pp}}$, which should make the experimenter rather confident about the estimated `posterior`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5caf4e3-a0cf-4776-9397-90d6d7fe4c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pairplot(\n",
    "    samples=x_pp,\n",
    "    points=x_o[0],\n",
    "    limits=torch.tensor([[-2.0, 5.0]] * 5),\n",
    "    points_colors=\"red\",\n",
    "    figsize=(8, 8),\n",
    "    offdiag=\"scatter\",\n",
    "    scatter_offdiag=dict(marker=\".\", s=5),\n",
    "    points_offdiag=dict(marker=\"+\", markersize=20),\n",
    "    labels=[rf\"$x_{d}$\" for d in range(D)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51d5c7c-3379-4d7d-adf1-638ff625dfbb",
   "metadata": {},
   "source": [
    "In contrast, $x_o$ falling well outside the support of $x_{\\text{pp}}$ is indicative of a failure to estimate the correct posterior. Here we simulate such a failure mode (by introducing a constant shift to the observations, which the neural estimator was not trained on):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c23e0e-d184-48c4-9ccc-c7d005924c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_shift = -2.0 * torch.ones(1, 5)\n",
    "\n",
    "_ = pairplot(\n",
    "    samples=x_pp,\n",
    "    points=x_o[0] + error_shift, # shift the observations\n",
    "    limits=torch.tensor([[-2.0, 5.0]] * 5),\n",
    "    points_colors=\"red\",\n",
    "    figsize=(8, 8),\n",
    "    offdiag=\"scatter\",\n",
    "    scatter_offdiag=dict(marker=\".\", s=5),\n",
    "    points_offdiag=dict(marker=\"+\", markersize=20),\n",
    "    labels=[rf\"$x_{d}$\" for d in range(D)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa024ca-54c9-40e7-8bd7-d35dea76569e",
   "metadata": {},
   "source": [
    "A typical way to investigate this issue would be to run a ***prior* predictive check**, applying the same plotting strategy, but drawing $\\theta$ from the prior instead of the posterior. **The support for $x_{\\text{pp}}$ should be larger and should contain $x_o$**. If this check is successful, the \"blame\" can then be shifted to the inference (method used, convergence of density estimators, number of sequential rounds, etc...)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
